{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fcbdd6",
   "metadata": {},
   "source": [
    "# Project 4: West Nile Virus Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffb5e9",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Data](#Data)\n",
    "- [Model](#Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf00d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import (RidgeCV, \n",
    "                                  LassoCV, \n",
    "                                  ElasticNetCV, \n",
    "                                  LogisticRegressionCV,\n",
    "                                  LinearRegression,\n",
    "                                  LogisticRegression)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             plot_confusion_matrix, \n",
    "                             roc_auc_score, \n",
    "                             plot_roc_curve, \n",
    "                             accuracy_score)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23575ae6",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "**Data** \n",
    "* train_set.csv: this is the training dataset\n",
    "* test_set.csv: this is the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0bca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../data/train_set.csv')\n",
    "train = pd.read_csv('../datasets/clean-data/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fead57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('../data/test_set.csv')\n",
    "test = pd.read_csv('../datasets/clean-data/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867e5278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>species_nr</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>addressaccuracy</th>\n",
       "      <th>nummosquitos</th>\n",
       "      <th>wnvpresent</th>\n",
       "      <th>trap_rank</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>...</th>\n",
       "      <th>avgspeed</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>FG</th>\n",
       "      <th>TS</th>\n",
       "      <th>lag_1_tavg</th>\n",
       "      <th>lag_2_tavg</th>\n",
       "      <th>lag_1_preciptotal</th>\n",
       "      <th>lag_2_preciptotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.688324</td>\n",
       "      <td>-87.676709</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.720848</td>\n",
       "      <td>-87.666014</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.731922</td>\n",
       "      <td>-87.677512</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.732984</td>\n",
       "      <td>-87.649642</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.862292</td>\n",
       "      <td>-87.648860</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.95</td>\n",
       "      <td>10.25</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>2013-09-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.960616</td>\n",
       "      <td>-87.777189</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>61.50</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8457</th>\n",
       "      <td>2013-09-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.964242</td>\n",
       "      <td>-87.757639</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>61.50</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8458</th>\n",
       "      <td>2013-09-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.973845</td>\n",
       "      <td>-87.805059</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>61.50</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8459</th>\n",
       "      <td>2013-09-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.974689</td>\n",
       "      <td>-87.890615</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>61.50</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8460</th>\n",
       "      <td>2013-09-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.984809</td>\n",
       "      <td>-87.728492</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.00</td>\n",
       "      <td>61.50</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8461 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  species_nr   latitude  longitude  addressaccuracy  \\\n",
       "0     2007-05-29         1.0  41.688324 -87.676709                8   \n",
       "1     2007-05-29         1.0  41.720848 -87.666014                9   \n",
       "2     2007-05-29         1.0  41.731922 -87.677512                8   \n",
       "3     2007-05-29         1.0  41.732984 -87.649642                8   \n",
       "4     2007-05-29         1.0  41.862292 -87.648860                8   \n",
       "...          ...         ...        ...        ...              ...   \n",
       "8456  2013-09-26         3.0  41.960616 -87.777189                9   \n",
       "8457  2013-09-26         3.0  41.964242 -87.757639                8   \n",
       "8458  2013-09-26         3.0  41.973845 -87.805059                9   \n",
       "8459  2013-09-26         3.0  41.974689 -87.890615                9   \n",
       "8460  2013-09-26         3.0  41.984809 -87.728492                8   \n",
       "\n",
       "      nummosquitos  wnvpresent  trap_rank  tmax  tmin  ...  avgspeed  \\\n",
       "0                1           0          0  88.0  62.5  ...      6.95   \n",
       "1                3           0          0  88.0  62.5  ...      6.95   \n",
       "2                5           0          0  88.0  62.5  ...      6.95   \n",
       "3                1           0          0  88.0  62.5  ...      6.95   \n",
       "4                1           0          0  88.0  62.5  ...      6.95   \n",
       "...            ...         ...        ...   ...   ...  ...       ...   \n",
       "8456             2           0          0  75.0  52.5  ...      4.40   \n",
       "8457             3           1          1  75.0  52.5  ...      4.40   \n",
       "8458             1           0          0  75.0  52.5  ...      4.40   \n",
       "8459            37           0          0  75.0  52.5  ...      4.40   \n",
       "8460             5           0          0  75.0  52.5  ...      4.40   \n",
       "\n",
       "      temp_diff  week    year   FG   TS  lag_1_tavg  lag_2_tavg  \\\n",
       "0         10.25  22.0  2007.0  0.0  0.0       56.25       73.25   \n",
       "1         10.25  22.0  2007.0  0.0  0.0       56.25       73.25   \n",
       "2         10.25  22.0  2007.0  0.0  0.0       56.25       73.25   \n",
       "3         10.25  22.0  2007.0  0.0  0.0       56.25       73.25   \n",
       "4         10.25  22.0  2007.0  0.0  0.0       56.25       73.25   \n",
       "...         ...   ...     ...  ...  ...         ...         ...   \n",
       "8456      -1.25  39.0  2013.0  0.0  0.0       70.00       61.50   \n",
       "8457      -1.25  39.0  2013.0  0.0  0.0       70.00       61.50   \n",
       "8458      -1.25  39.0  2013.0  0.0  0.0       70.00       61.50   \n",
       "8459      -1.25  39.0  2013.0  0.0  0.0       70.00       61.50   \n",
       "8460      -1.25  39.0  2013.0  0.0  0.0       70.00       61.50   \n",
       "\n",
       "      lag_1_preciptotal  lag_2_preciptotal  \n",
       "0                 0.345                0.0  \n",
       "1                 0.345                0.0  \n",
       "2                 0.345                0.0  \n",
       "3                 0.345                0.0  \n",
       "4                 0.345                0.0  \n",
       "...                 ...                ...  \n",
       "8456              0.460                0.0  \n",
       "8457              0.460                0.0  \n",
       "8458              0.460                0.0  \n",
       "8459              0.460                0.0  \n",
       "8460              0.460                0.0  \n",
       "\n",
       "[8461 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview train set\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0296b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8461, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0830d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8461 entries, 0 to 8460\n",
      "Data columns (total 31 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   date               8461 non-null   object \n",
      " 1   species_nr         8461 non-null   float64\n",
      " 2   latitude           8461 non-null   float64\n",
      " 3   longitude          8461 non-null   float64\n",
      " 4   addressaccuracy    8461 non-null   int64  \n",
      " 5   nummosquitos       8461 non-null   int64  \n",
      " 6   wnvpresent         8461 non-null   int64  \n",
      " 7   trap_rank          8461 non-null   int64  \n",
      " 8   tmax               8461 non-null   float64\n",
      " 9   tmin               8461 non-null   float64\n",
      " 10  tavg               8461 non-null   float64\n",
      " 11  depart             8461 non-null   float64\n",
      " 12  dewpoint           8461 non-null   float64\n",
      " 13  wetbulb            8461 non-null   float64\n",
      " 14  sunrise            8461 non-null   float64\n",
      " 15  sunset             8461 non-null   float64\n",
      " 16  preciptotal        8461 non-null   float64\n",
      " 17  stnpressure        8461 non-null   float64\n",
      " 18  sealevel           8461 non-null   float64\n",
      " 19  resultspeed        8461 non-null   float64\n",
      " 20  resultdir          8461 non-null   float64\n",
      " 21  avgspeed           8461 non-null   float64\n",
      " 22  temp_diff          8461 non-null   float64\n",
      " 23  week               8461 non-null   float64\n",
      " 24  year               8461 non-null   float64\n",
      " 25  FG                 8461 non-null   float64\n",
      " 26  TS                 8461 non-null   float64\n",
      " 27  lag_1_tavg         8461 non-null   float64\n",
      " 28  lag_2_tavg         8461 non-null   float64\n",
      " 29  lag_1_preciptotal  8461 non-null   float64\n",
      " 30  lag_2_preciptotal  8461 non-null   float64\n",
      "dtypes: float64(26), int64(4), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ceb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date to numerical\n",
    "train['date'] = pd.to_datetime(train['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e8bce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>addressaccuracy</th>\n",
       "      <th>species_nr</th>\n",
       "      <th>trap_rank</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tavg</th>\n",
       "      <th>...</th>\n",
       "      <th>avgspeed</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>week</th>\n",
       "      <th>year</th>\n",
       "      <th>FG</th>\n",
       "      <th>TS</th>\n",
       "      <th>lag_1_tavg</th>\n",
       "      <th>lag_2_tavg</th>\n",
       "      <th>lag_1_preciptotal</th>\n",
       "      <th>lag_2_preciptotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-06-11</td>\n",
       "      <td>41.95469</td>\n",
       "      <td>-87.800991</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>74.75</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2008-06-11</td>\n",
       "      <td>41.95469</td>\n",
       "      <td>-87.800991</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>74.75</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2008-06-11</td>\n",
       "      <td>41.95469</td>\n",
       "      <td>-87.800991</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>74.75</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2008-06-11</td>\n",
       "      <td>41.95469</td>\n",
       "      <td>-87.800991</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>74.75</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2008-06-11</td>\n",
       "      <td>41.95469</td>\n",
       "      <td>-87.800991</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>63.5</td>\n",
       "      <td>74.75</td>\n",
       "      <td>...</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.75</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  latitude  longitude  addressaccuracy  species_nr  \\\n",
       "0   1  2008-06-11  41.95469 -87.800991                9         2.0   \n",
       "1   2  2008-06-11  41.95469 -87.800991                9         1.0   \n",
       "2   3  2008-06-11  41.95469 -87.800991                9         3.0   \n",
       "3   4  2008-06-11  41.95469 -87.800991                9         0.0   \n",
       "4   5  2008-06-11  41.95469 -87.800991                9         0.0   \n",
       "\n",
       "   trap_rank  tmax  tmin   tavg  ...  avgspeed  temp_diff  week    year   FG  \\\n",
       "0          0  86.0  63.5  74.75  ...      10.2       9.75  24.0  2008.0  0.0   \n",
       "1          0  86.0  63.5  74.75  ...      10.2       9.75  24.0  2008.0  0.0   \n",
       "2          0  86.0  63.5  74.75  ...      10.2       9.75  24.0  2008.0  0.0   \n",
       "3          0  86.0  63.5  74.75  ...      10.2       9.75  24.0  2008.0  0.0   \n",
       "4          0  86.0  63.5  74.75  ...      10.2       9.75  24.0  2008.0  0.0   \n",
       "\n",
       "    TS  lag_1_tavg  lag_2_tavg  lag_1_preciptotal  lag_2_preciptotal  \n",
       "0  0.0        66.5       72.75                0.0              0.095  \n",
       "1  0.0        66.5       72.75                0.0              0.095  \n",
       "2  0.0        66.5       72.75                0.0              0.095  \n",
       "3  0.0        66.5       72.75                0.0              0.095  \n",
       "4  0.0        66.5       72.75                0.0              0.095  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview test set\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad73ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914edf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116293 entries, 0 to 116292\n",
      "Data columns (total 30 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 116293 non-null  int64  \n",
      " 1   date               116293 non-null  object \n",
      " 2   latitude           116293 non-null  float64\n",
      " 3   longitude          116293 non-null  float64\n",
      " 4   addressaccuracy    116293 non-null  int64  \n",
      " 5   species_nr         116293 non-null  float64\n",
      " 6   trap_rank          116293 non-null  int64  \n",
      " 7   tmax               116293 non-null  float64\n",
      " 8   tmin               116293 non-null  float64\n",
      " 9   tavg               116293 non-null  float64\n",
      " 10  depart             116293 non-null  float64\n",
      " 11  dewpoint           116293 non-null  float64\n",
      " 12  wetbulb            116293 non-null  float64\n",
      " 13  sunrise            116293 non-null  float64\n",
      " 14  sunset             116293 non-null  float64\n",
      " 15  preciptotal        116293 non-null  float64\n",
      " 16  stnpressure        116293 non-null  float64\n",
      " 17  sealevel           116293 non-null  float64\n",
      " 18  resultspeed        116293 non-null  float64\n",
      " 19  resultdir          116293 non-null  float64\n",
      " 20  avgspeed           116293 non-null  float64\n",
      " 21  temp_diff          116293 non-null  float64\n",
      " 22  week               116293 non-null  float64\n",
      " 23  year               116293 non-null  float64\n",
      " 24  FG                 116293 non-null  float64\n",
      " 25  TS                 116293 non-null  float64\n",
      " 26  lag_1_tavg         116293 non-null  float64\n",
      " 27  lag_2_tavg         116293 non-null  float64\n",
      " 28  lag_1_preciptotal  116293 non-null  float64\n",
      " 29  lag_2_preciptotal  116293 non-null  float64\n",
      "dtypes: float64(26), int64(3), object(1)\n",
      "memory usage: 26.6+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff6ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date to numerical\n",
    "test['date'] = pd.to_datetime(test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bed380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8004\n",
       "1      409\n",
       "2       31\n",
       "3        9\n",
       "4        2\n",
       "8        1\n",
       "9        1\n",
       "10       1\n",
       "5        1\n",
       "6        1\n",
       "7        1\n",
       "Name: wnvpresent, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline score\n",
    "train['wnvpresent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52c545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary\n",
    "# 0 means no wnv detected\n",
    "# 1 means wnv detected\n",
    "def rebin_wnvpresent(row): \n",
    "    if row['wnvpresent'] > 0:\n",
    "        row['wnvpresent'] = 1\n",
    "    else:\n",
    "        row['wnvpresent'] = 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "404f48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(rebin_wnvpresent,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e42c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8004\n",
       "1     457\n",
       "Name: wnvpresent, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['wnvpresent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142b120",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebead4",
   "metadata": {},
   "source": [
    "Any of these features could be related to each other, or have an interactive effect with each other. Any synergistic effect on wnvpresent could increase the significant predictive power to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125f4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'nummosquitos' was removed from polynomial features because it is not a feature in the test set\n",
    "# 'date' was removed as we extracted the 'week', 'year' from it.\n",
    "X = train.drop(columns=['date','wnvpresent','nummosquitos'])\n",
    "y = train['wnvpresent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54c23f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8461, 434)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating the polynomial features table.  \n",
    "# instantiate\n",
    "poly = PolynomialFeatures(include_bias=False, degree=2)\n",
    "\n",
    "# fit and transform the variables in the numerical dataframe\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b3cbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addressaccuracy trap_rank      0.915657\n",
       "latitude trap_rank             0.912398\n",
       "trap_rank year                 0.912283\n",
       "trap_rank                      0.912233\n",
       "trap_rank sealevel             0.912162\n",
       "trap_rank stnpressure          0.912148\n",
       "trap_rank sunset               0.911941\n",
       "trap_rank week                 0.910168\n",
       "trap_rank sunrise              0.908579\n",
       "trap_rank tavg                 0.907355\n",
       "trap_rank tmin                 0.907185\n",
       "trap_rank wetbulb              0.907144\n",
       "trap_rank lag_2_tavg           0.906520\n",
       "trap_rank tmax                 0.906483\n",
       "trap_rank dewpoint             0.904773\n",
       "trap_rank lag_1_tavg           0.904692\n",
       "trap_rank avgspeed             0.871990\n",
       "species_nr trap_rank           0.853837\n",
       "trap_rank resultspeed          0.827764\n",
       "trap_rank resultdir            0.821981\n",
       "trap_rank temp_diff            0.792499\n",
       "trap_rank^2                    0.579206\n",
       "trap_rank depart               0.571068\n",
       "trap_rank preciptotal          0.474681\n",
       "trap_rank TS                   0.461440\n",
       "trap_rank lag_2_preciptotal    0.456916\n",
       "trap_rank lag_1_preciptotal    0.292001\n",
       "trap_rank FG                   0.235388\n",
       "species_nr dewpoint            0.150687\n",
       "tmin sunrise                   0.148456\n",
       "wetbulb sunrise                0.147635\n",
       "species_nr tmin                0.147299\n",
       "dewpoint week                  0.146243\n",
       "dewpoint sunrise               0.146148\n",
       "species_nr wetbulb             0.145775\n",
       "tmin week                      0.145541\n",
       "wetbulb week                   0.143137\n",
       "species_nr tavg                0.141766\n",
       "species_nr week                0.139369\n",
       "tavg sunrise                   0.138316\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking column names to all polynomial features\n",
    "X_poly = pd.DataFrame(X_poly,columns=poly.get_feature_names(X.columns))\n",
    "\n",
    "# Generating list of poly feature correlations\n",
    "X_poly_corrs = X_poly.corrwith(y)\n",
    "\n",
    "# Shows top 40 features most positively correlated with wnvpresent\n",
    "X_poly_corrs.sort_values(ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451dc38",
   "metadata": {},
   "source": [
    "The above showed that the highest correlation seemed to be location (trap_rank and addressaccuracy) related. From EDA, we already know that trap_rank has high correlation, and might not be necessary to add another feature related to trap_rank. In addition, nummosquitoes is not in the test set. So we think adding more features related to temperature such as 'species_nr dewpoint', 'tmin sunrise', 'wetbulb sunrise' would increase the predictive power of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9ac81",
   "metadata": {},
   "source": [
    "It is also important to note features that might have the highest negative impact on wnvpresent. The below list shows that longitude and sunset might have decrease/discourage the presence of wnvpresent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0cadd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude TS           -0.056869\n",
       "sunset year            -0.057391\n",
       "sunset                 -0.059019\n",
       "sunset stnpressure     -0.060200\n",
       "sunset sealevel        -0.060779\n",
       "longitude tmax         -0.061344\n",
       "sunset^2               -0.061918\n",
       "longitude depart       -0.065620\n",
       "longitude              -0.076600\n",
       "longitude FG           -0.077753\n",
       "longitude temp_diff    -0.078773\n",
       "longitude tavg         -0.079429\n",
       "longitude year         -0.080058\n",
       "longitude tmin         -0.090030\n",
       "longitude wetbulb      -0.092972\n",
       "longitude dewpoint     -0.095661\n",
       "longitude sunrise      -0.098397\n",
       "longitude week         -0.101082\n",
       "species_nr longitude   -0.125671\n",
       "longitude trap_rank    -0.912227\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows bottom 20 features most positively correlated with wnvpresent\n",
    "X_poly_corrs.sort_values(ascending=False).tail(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e3da59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction features into train set\n",
    "train['species_nr*dewpoint'] = train['species_nr'] * train['dewpoint']\n",
    "train['tmin*sunrise'] = train['tmin'] * train['sunrise']\n",
    "train['wetbulb*sunrise'] = train['wetbulb'] * train['sunrise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc58a36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>species_nr</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>addressaccuracy</th>\n",
       "      <th>nummosquitos</th>\n",
       "      <th>wnvpresent</th>\n",
       "      <th>trap_rank</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>FG</th>\n",
       "      <th>TS</th>\n",
       "      <th>lag_1_tavg</th>\n",
       "      <th>lag_2_tavg</th>\n",
       "      <th>lag_1_preciptotal</th>\n",
       "      <th>lag_2_preciptotal</th>\n",
       "      <th>species_nr*dewpoint</th>\n",
       "      <th>tmin*sunrise</th>\n",
       "      <th>wetbulb*sunrise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.688324</td>\n",
       "      <td>-87.676709</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>26312.5</td>\n",
       "      <td>27575.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.720848</td>\n",
       "      <td>-87.666014</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>26312.5</td>\n",
       "      <td>27575.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.731922</td>\n",
       "      <td>-87.677512</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>26312.5</td>\n",
       "      <td>27575.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.732984</td>\n",
       "      <td>-87.649642</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>26312.5</td>\n",
       "      <td>27575.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-05-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.862292</td>\n",
       "      <td>-87.648860</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>73.25</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>26312.5</td>\n",
       "      <td>27575.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  species_nr   latitude  longitude  addressaccuracy  nummosquitos  \\\n",
       "0 2007-05-29         1.0  41.688324 -87.676709                8             1   \n",
       "1 2007-05-29         1.0  41.720848 -87.666014                9             3   \n",
       "2 2007-05-29         1.0  41.731922 -87.677512                8             5   \n",
       "3 2007-05-29         1.0  41.732984 -87.649642                8             1   \n",
       "4 2007-05-29         1.0  41.862292 -87.648860                8             1   \n",
       "\n",
       "   wnvpresent  trap_rank  tmax  tmin  ...    year   FG   TS  lag_1_tavg  \\\n",
       "0           0          0  88.0  62.5  ...  2007.0  0.0  0.0       56.25   \n",
       "1           0          0  88.0  62.5  ...  2007.0  0.0  0.0       56.25   \n",
       "2           0          0  88.0  62.5  ...  2007.0  0.0  0.0       56.25   \n",
       "3           0          0  88.0  62.5  ...  2007.0  0.0  0.0       56.25   \n",
       "4           0          0  88.0  62.5  ...  2007.0  0.0  0.0       56.25   \n",
       "\n",
       "   lag_2_tavg  lag_1_preciptotal  lag_2_preciptotal  species_nr*dewpoint  \\\n",
       "0       73.25              0.345                0.0                 58.5   \n",
       "1       73.25              0.345                0.0                 58.5   \n",
       "2       73.25              0.345                0.0                 58.5   \n",
       "3       73.25              0.345                0.0                 58.5   \n",
       "4       73.25              0.345                0.0                 58.5   \n",
       "\n",
       "   tmin*sunrise  wetbulb*sunrise  \n",
       "0       26312.5          27575.5  \n",
       "1       26312.5          27575.5  \n",
       "2       26312.5          27575.5  \n",
       "3       26312.5          27575.5  \n",
       "4       26312.5          27575.5  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final train set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cb89891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction features into test set\n",
    "test['species_nr*dewpoint'] = test['species_nr'] * test['dewpoint']\n",
    "test['tmin*sunrise'] = test['tmin'] * test['sunrise']\n",
    "test['wetbulb*sunrise'] = test['wetbulb'] * test['sunrise']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef2316",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffe3a2",
   "metadata": {},
   "source": [
    "## Baseline model (with SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c124515",
   "metadata": {},
   "source": [
    "Let's just start with modeling using one feature. Since 'trap_rank', 'species_nr' has the largest correlation to 'wnvpresent', the feature will be used. Simplest classification method would be the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "839ae1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm = train[['species_nr']]\n",
    "y_sm = train[['wnvpresent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468bf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "Xsm_train, Xsm_valid, ysm_train, ysm_valid = train_test_split(X_sm, y_sm, test_size = 0.33, random_state = 42, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be4ecb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "ss = StandardScaler()\n",
    "\n",
    "Xsm_train_sc = ss.fit_transform(Xsm_train)\n",
    "Xsm_valid_sc = ss.transform(Xsm_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c24299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9460127028934369, 0.9459362692445399)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "lr.fit(Xsm_train_sc, ysm_train)\n",
    "\n",
    "# score\n",
    "lr.score(Xsm_train_sc, ysm_train), lr.score(Xsm_valid_sc, ysm_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f15a023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wnvpresent\n",
       "0             5362\n",
       "1              306\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ysm_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee2c48",
   "metadata": {},
   "source": [
    "Using only one feature, regardless which one feature, would result in high R2 score. This is because the predictive feature is imbalanced in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7d7f8",
   "metadata": {},
   "source": [
    "**with SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b42484c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic data for training set\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "# Xsmote_train, ysmote_train = smote.fit_sample(Xsm_train_sc, ysm_train)\n",
    "Xsmote_train, ysmote_train = smote.fit_resample(Xsm_train_sc, ysm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ysmote_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf10f0c",
   "metadata": {},
   "source": [
    "Now, the predictive feature is more balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "lr2 = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "lr2.fit(Xsmote_train, ysmote_train)\n",
    "\n",
    "# score\n",
    "lr2.score(Xsmote_train, ysmote_train), lr2.score(Xsm_valid_sc, ysm_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28433a7",
   "metadata": {},
   "source": [
    "### Logistic regression model (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of train with polynomial features\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f486c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['date','wnvpresent','nummosquitos'])\n",
    "y = train[['wnvpresent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('logreg', LogisticRegression(max_iter=1_000, solver='saga'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regardless of parameters, there is overfitting.\n",
    "pipe_params = {\n",
    "    'sampling__sampling_strategy': ['minority', 'not minority', 'auto'],\n",
    "    'sampling__k_neighbors': [250],   # tried 1-10000\n",
    "    'logreg__penalty': ['l2', 'l1'],   # tried 'l2', 'l1', 'elastinet'\n",
    "    'logreg__C': [1] # tried 1-20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, pipe_params, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5571f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_train, y_train), grid.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c03723",
   "metadata": {},
   "source": [
    "Regardless of pipe_params used, there is overfitting. There is a lot of multicollinearity, and features have to be dropped before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edb0b0",
   "metadata": {},
   "source": [
    "Since there might be too many variables with high multicollinearity, we need to reduce number of features and/or utilise PCA to transform the data first. In such a case, we can use PCA to identify and keep those potential important relationships, before performing the predictions/models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc9569",
   "metadata": {},
   "source": [
    "## Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6aceb",
   "metadata": {},
   "source": [
    "We conduct several models based on a reduced number of features.\n",
    "\n",
    "**Drop Column: ```Week``` & ```Date```**\n",
    "\n",
    "While our graphs had provided meaningful visualization as to the prevalence of mosquitos and presence of WNV in weekly periods, we have dropped this variable in testing our logistic models. Mosquitos do not adhere to our human-based calendar groupings such as 'week' and 'date'; their activities are influenced by weather patterns. Due to climate change, weather patterns have shifted such that static human-based calendar groupings are not a precise indicator of seasons. For instance, while 'Summer' starts on June 21, the weather in June 21, 1990 is different than that of June 21, 2021. As static human-based groupings do not affect mosquito life cycle and activity, we believe that excluding such features would provide us with more accurate results.\n",
    "\n",
    "We had also taken an into account our preliminary models in dropping these columns. In running our preliminary models including human-based calendar groupings, we found that as the most important feature, 'week' held a significant weight (143x) when compared to the second and third-most important feature, wetbulb and stnpressure (35x and 17x respectively). This means that as week increases by one, the probability of a trap capturing a WNV-infected mosquito is 143 times more likely, whereas an increase in one degree (Fahrenheit) in wetbulb or an increase of one inch in mercury-based measurement (Hg) results in a 35x and 17x likelihood of a WNV mosquito being present. Due to this massive difference in magnitude between the most important feature as compared to the rest, we believe that it is unwise to include the 'week' feature in our model.\n",
    "\n",
    "\n",
    "**Drop Columns: ```Wnvpresent``` & ```Nummosquitos```**\n",
    "\n",
    "We dropped the columns ```wnvpresent``` and ```nummosquitos``` as they are our target variables and should not be included in our training set.\n",
    "\n",
    "**Drop Columns: ```Tmin``` & ```Tmax```**\n",
    "\n",
    "We dropped the columns ```Tmin``` and ```Tmax``` due to multicollinearity with ```Tavg```. We believe that ```Tavg``` provides us a better gauge of the day's temperature rather than using the minimum and maximum temperature of the day.\n",
    "\n",
    "**Drop Column: ```trap_rank```**\n",
    "\n",
    "We dropped ```trap_rank``` as this is our engineered feature based on our target variables. Including this feature in our model would result in leakage to our data and this will result in an overfitting of our training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a675460",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['week', 'date', 'wnvpresent', 'nummosquitos',\n",
    "                        'tmin', 'tmax', 'trap_rank'])\n",
    "\n",
    "y = train['wnvpresent'].map(lambda x:0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0624afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7622e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_X = test[['species_nr', 'latitude', 'longitude', 'addressaccuracy', 'tavg', \n",
    "                 'depart', 'dewpoint', 'wetbulb', 'sunrise', 'sunset', 'preciptotal', \n",
    "                 'stnpressure', 'sealevel', 'resultspeed', 'resultdir', 'avgspeed', \n",
    "                 'temp_diff', 'year', 'FG', 'TS', 'lag_1_tavg', 'lag_2_tavg', \n",
    "                 'lag_1_preciptotal', 'lag_2_preciptotal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cf03f",
   "metadata": {},
   "source": [
    "### Loop models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result_columns = ['Model', 'Accuracy', 'Sensitivity', 'Specificity', 'ROC_AUC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb08256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_model(gs_name, model_name):\n",
    "    global result\n",
    "    duration = time.time()\n",
    "    gs_name.fit(X_train, y_train)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, gs_name.predict(X_test)).ravel()\n",
    "    accuracy = round((tp+tn)/(tn+fp+fn+tp),3)*100 \n",
    "    sensitivity = round((tp)/(tp+fn),3)*100\n",
    "    specificity = round(tn/(tn+fp),3)*100\n",
    "    roc_auc = round(roc_auc_score(y_test, gs_name.best_estimator_.predict(X_test)),3)\n",
    "    \n",
    "    result_new = pd.DataFrame([model_name, \n",
    "                               accuracy,\n",
    "                               sensitivity,\n",
    "                               specificity,\n",
    "                               roc_auc\n",
    "                               ], index=result_columns).T\n",
    "    \n",
    "    result = pd.concat(objs=(result, result_new), axis=0)\n",
    "    \n",
    "    plot_confusion_matrix(gs_name, \n",
    "                          X_test, y_test, \n",
    "                          cmap='Blues', \n",
    "                          values_format='d',\n",
    "                          display_labels=['No WNV', 'WNV']\n",
    "                         );\n",
    "    plt.title(model_name)\n",
    "    print(f'Model Best Params: {gs_name.best_params_}')\n",
    "    print(f'Process took {round((time.time()-duration)/60,1)} minutes')\n",
    "    return result.set_index('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128933a",
   "metadata": {},
   "source": [
    "### Logistic Classifier Models\n",
    "\n",
    "We conduct several classifier models based on different penalization methods and classifier models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876c7e1",
   "metadata": {},
   "source": [
    "#### SS, Smote, Logistic (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ed688",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "ss = StandardScaler()\n",
    "logit = LogisticRegressionCV()\n",
    "\n",
    "pipe_ss_smote_logit_ridge = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('smote', smote),\n",
    "    ('logit', logit),\n",
    "])\n",
    "\n",
    "pipe_ss_smote_logit_ridge_params = {\n",
    "    'smote__k_neighbors': [5], # 3, 10, 15\n",
    "    'smote__random_state': [42],\n",
    "    'smote__n_jobs': [-1],\n",
    "    'smote__sampling_strategy': ['minority'],\n",
    "    'logit__cv': [15], #5, 10, 20, 25\n",
    "    'logit__n_jobs': [-1],\n",
    "    'logit__random_state': [42],\n",
    "    'logit__scoring': ['roc_auc'],\n",
    "    'logit__solver': ['saga'],\n",
    "    'logit__max_iter': [10_000],\n",
    "}\n",
    "\n",
    "gs_ss_smote_logit_ridge = GridSearchCV(\n",
    "    estimator=pipe_ss_smote_logit_ridge,\n",
    "    param_grid=pipe_ss_smote_logit_ridge_params,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b67ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model(gs_ss_smote_logit_ridge, 'SS, Smote, Logit-Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1565cbc",
   "metadata": {},
   "source": [
    "**Observations and Interpretation of Feature Importance**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c562316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_impt = pd.DataFrame(\n",
    "    gs_ss_smote_logit_ridge.best_estimator_['logit'].coef_[0], \n",
    "    index=X_train.columns)\n",
    "\n",
    "logit_ridge_head = pd.DataFrame(np.exp(feat_impt[0].sort_values(ascending=False).head(5))).T\n",
    "logit_ridge_tail = pd.DataFrame(np.exp(feat_impt[0].sort_values(ascending=False).tail(5))).T\n",
    "\n",
    "display(logit_ridge_head, logit_ridge_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe01595",
   "metadata": {},
   "source": [
    "We find that the features we used to create out models have differing magnitudes of impact.\n",
    "As the sun rises later, there is a much higher likelihood of observing a WNV-infected mosquito.\n",
    "In any given day, an increase in one minute in the sunrise time results in a 13.6x likelihood of a WNV-infected mosquito to be found.\n",
    "An increase of 1 Fahrenheit in average temperature results in a 5.3x likelihood of a WNV-infected mosquito to be found.\n",
    "Precipitation negatively affects the likelihood of observing a WNV-infected mosquito.\n",
    "An increase in precipitation by 1 inch results in a 0.67x likelihood of a WNV-infected mosquito to be found.\n",
    "The positive relationship in average temperature and inverse relationship in precipitation is similar to that we have identified in our previous EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597cb11",
   "metadata": {},
   "source": [
    "#### SS, Smote, Logistic (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e88e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "ss = StandardScaler()\n",
    "logit = LogisticRegressionCV()\n",
    "\n",
    "pipe_ss_smote_logit_lasso = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('smote', smote),\n",
    "    ('logit', logit),\n",
    "])\n",
    "\n",
    "pipe_ss_smote_logit_lasso_params = {\n",
    "    'smote__k_neighbors': [3], # 5, 10, 15\n",
    "    'smote__random_state': [42],\n",
    "    'smote__n_jobs': [-1],\n",
    "    'smote__sampling_strategy': ['minority'],\n",
    "    'logit__cv': [20], # 5, 10, 15, 25\n",
    "    'logit__n_jobs': [-1],\n",
    "    'logit__random_state': [42],\n",
    "    'logit__scoring': ['roc_auc'],\n",
    "    'logit__solver': ['saga'],\n",
    "    'logit__max_iter': [10_000],\n",
    "    'logit__penalty': ['l1'],\n",
    "#     'logit__l1_ratios': [num for num in np.logspace(-3, 3, 20)]\n",
    "}\n",
    "\n",
    "gs_ss_smote_logit_lasso = GridSearchCV(\n",
    "    estimator=pipe_ss_smote_logit_lasso,\n",
    "    param_grid=pipe_ss_smote_logit_lasso_params,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model(gs_ss_smote_logit_lasso, 'SS, Smote, Logit-Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7cb052",
   "metadata": {},
   "source": [
    "**Observations and Interpretation of Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_impt = pd.DataFrame(\n",
    "    gs_ss_smote_logit_lasso.best_estimator_['logit'].coef_[0], \n",
    "    index=X_train.columns)\n",
    "\n",
    "logit_lasso_head = pd.DataFrame(np.exp(feat_impt[0].sort_values(ascending=False).head(5))).T\n",
    "logit_lasso_tail = pd.DataFrame(np.exp(feat_impt[0].sort_values(ascending=False).tail(5))).T\n",
    "\n",
    "display(logit_lasso_head, logit_lasso_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8047d",
   "metadata": {},
   "source": [
    "We find that the features we used to create out models have differing magnitudes of impact.\n",
    "As the sun rises later, there is a much higher likelihood of observing a WNV-infected mosquito.\n",
    "In any given day, an increase in one minute in the sunrise time results in a 13x likelihood of a WNV-infected mosquito to be found.\n",
    "An increase of 1 Fahrenheit in average temperature results in a 5.5x likelihood of a WNV-infected mosquito to be found.\n",
    "Precipitation negatively affects the likelihood of observing a WNV-infected mosquito.\n",
    "An increase in precipitation by 1 inch results in a 0.67x likelihood of a WNV-infected mosquito to be found.\n",
    "The positive relationship in average temperature and inverse relationship in precipitation is similar to that we have identified in our previous EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea903ed",
   "metadata": {},
   "source": [
    "#### Logistic Classifier Model Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_combined_head = pd.concat(objs=(logit_ridge_head, logit_lasso_head), axis=0).reset_index(drop=True)\n",
    "logit_combined_head.index = ['Ridge Head', 'Lasso Head']\n",
    "\n",
    "logit_combined_tail = pd.concat(objs=(logit_ridge_tail, logit_lasso_tail), axis=0).reset_index(drop=True)\n",
    "logit_combined_tail.index = ['Ridge Tail', 'Lasso Tail']\n",
    "\n",
    "display(logit_combined_head)\n",
    "display(logit_combined_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b8eef",
   "metadata": {},
   "source": [
    "**Feature Importance Comparison**\n",
    "\n",
    "We compare the feature importances of the top 5 heads and tails between our model.\n",
    "\n",
    "We find that 4 out of 5 of the top head features used in the logistic classification model are identical.\n",
    "However, there is a difference in magnitude of effect. The magnitudes in the lasso features are more evenly distributed than the ridge feature importances.\n",
    "The difference in magnitude is caused by the different types of penalization between the two regularization models.\n",
    "We find that all 5 of the top tail features used in the logistic classification model are identical.\n",
    "However, similar to #2, there is a difference in the magnitude of effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583ded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result.set_index('Model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f480f2",
   "metadata": {},
   "source": [
    "**Model Comparison**\n",
    "\n",
    "We observe the metrics in our observations and find that our L1 penalized model consistently performs better in our metrics of concern: Specificity and ROC_AUC. As we expand our model to include other potential metrics that may be of relevance, we find that our lasso model still performs better for accuracy and specificity. Running our models on Kaggle for testing, we obtain the following result:\n",
    "\n",
    "|Model|Sensitivity|ROC_AUC|Kaggle Score|\n",
    "|-----|-----------|-------|------------|\n",
    "|SS, Smote, Logit-Ridge|0.725|0.731|0.71532|\n",
    "|SS, Smote, Logit-Lasso|0.731|0.738|0.72462|\n",
    "\n",
    "Knowing this, we believe that our lasso model is the preferred logistic classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f8fa4",
   "metadata": {},
   "source": [
    "### Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7c8b2",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "pipe_ss_smote_dt = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('smote', smote),\n",
    "    ('dt', dt)\n",
    "])\n",
    "\n",
    "pipe_ss_smote_dt_params = {\n",
    "    'ss__with_mean': [True, False], \n",
    "    'ss__with_std': [True, False],\n",
    "    'smote__k_neighbors': [7, 9, 11, 15], # 3, 5\n",
    "    'smote__random_state': [42],\n",
    "    'smote__n_jobs': [-1],\n",
    "    'smote__sampling_strategy': ['minority'],\n",
    "    'dt__max_depth': [15, 20, 25, 30], # 5, 10\n",
    "    'dt__max_features': [0.9, 1.0], # 0.5, 0.75\n",
    "    'dt__random_state': [42],\n",
    "#     'dt__min__samples_split': [2, 5, 8],\n",
    "#     'dt__min_impurity_split': [0.02, 0.04, 0.06]\n",
    "}\n",
    "\n",
    "gs_ss_smote_dt = GridSearchCV(\n",
    "    estimator=pipe_ss_smote_dt,\n",
    "    param_grid=pipe_ss_smote_dt_params,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20609773",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model(gs_ss_smote_dt, 'SS, Smote, DTree')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be3181",
   "metadata": {},
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier()\n",
    "\n",
    "pipe_ss_smote_bag = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('smote', smote),\n",
    "    ('bag', bag)\n",
    "])\n",
    "\n",
    "pipe_ss_smote_bag_params = {\n",
    "    'ss__with_mean': [True, False], \n",
    "    'ss__with_std': [True, False],\n",
    "    'smote__k_neighbors': [9], # 3, 5, 7, 11, 15\n",
    "    'smote__random_state': [42],\n",
    "    'smote__n_jobs': [-1],\n",
    "    'smote__sampling_strategy': ['minority'],\n",
    "    'bag__random_state': [42],\n",
    "    'bag__max_features': [0.5], # 0.5\n",
    "    'bag__n_jobs': [-1],\n",
    "    'bag__max_samples': [0.7],    # 0.8, 1.0\n",
    "}\n",
    "\n",
    "gs_ss_smote_bag = GridSearchCV(\n",
    "    estimator=pipe_ss_smote_bag,\n",
    "    param_grid=pipe_ss_smote_bag_params,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model(gs_ss_smote_bag, 'SS, Smote, Bag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405f287",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "pipe_ss_smote_xgb = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('smote', smote),\n",
    "    ('xgb', xgb)\n",
    "])\n",
    "\n",
    "pipe_ss_smote_xgb_params = {\n",
    "    'ss__with_mean': [True, False], \n",
    "    'ss__with_std': [True, False],\n",
    "    'smote__k_neighbors': [7, 9, 11, 13],\n",
    "    'smote__random_state': [42],\n",
    "    'smote__n_jobs': [-1],\n",
    "    'smote__sampling_strategy': ['minority'],\n",
    "    'xgb__random_state': [42],\n",
    "    'xgb__n_jobs': [-1],\n",
    "    'xgb__learning_rate': [0.1, 0.2, 0.3],\n",
    "    'xgb__objective': ['binary:logistic'],\n",
    "    'xgb__n_estimators': [100, 300, 500],\n",
    "    'xgb__eval_metric': ['auc']\n",
    "}\n",
    "\n",
    "gs_ss_smote_xgb = GridSearchCV(\n",
    "    estimator=pipe_ss_smote_xgb,\n",
    "    param_grid=pipe_ss_smote_xgb_params,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model(gs_ss_smote_xgb, 'SS, Smote, XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ce8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BREAKKKKKKKKKK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13c78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889547d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc74bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61b131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb73f3fb",
   "metadata": {},
   "source": [
    "### Run Model for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d563f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = time.time()\n",
    "kaggle_submit = []\n",
    "kk = 0\n",
    "def kaggle_test(gs_model):\n",
    "    for i in range(kaggle_X.shape[0]):\n",
    "        kaggle_submit.extend([gs_model.best_estimator_.predict_proba(kaggle_X)[i][1]])\n",
    "        if i % 2000 == 0:\n",
    "            print(kk)\n",
    "            print(f'Total time elapsed: {round((time.time()-duration)/60,2)} mins')\n",
    "            kk += 1\n",
    "\n",
    "    print(f'Total completion time: {round((time.time() - duration)/60,2)} mins')\n",
    "    \n",
    "    kaggle_submit = pd.DataFrame(kaggle_submit)\n",
    "    kaggle_submit.reset_index(inplace=True)\n",
    "    kaggle_submit.columns = ['Id', 'WnvPresent']\n",
    "    kaggle_submit['Id'] = kaggle_submit['Id'] + 1\n",
    "    kaggle_submit.set_index('Id', inplace=True)\n",
    "    \n",
    "    return kaggle_submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and fix pathway to run and save above models.\n",
    "# kaggle_test(gs_ss_smote_logit_lasso)\n",
    "# kaggle_submit.to_csv(r'../modeling results/gs_ss_smote_logit_lasso.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b4990",
   "metadata": {},
   "source": [
    "## Feature extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1acef",
   "metadata": {},
   "source": [
    "### Exploring PCA-transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c353b",
   "metadata": {},
   "source": [
    "Due to the number of features (and high-dimensional manifold) in our dataset, PCA was utilised to perform an unsupervised dimensionality reduction on the training dataset. In such a way, PCA will provide the best linear approximations before modeling and prediction was performed ([source](https://arxiv.org/ftp/arxiv/papers/1403/1403.1949.pdf)). GridSearchCV is used to set the dimensionality of the PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eab287",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['date','wnvpresent','nummosquitos'])\n",
    "y = train[['wnvpresent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split again, just to make sure\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify =y)\n",
    "\n",
    "# scaling\n",
    "ss_pca = StandardScaler()\n",
    "\n",
    "X_train_sc = ss_pca.fit_transform(X_train)\n",
    "X_valid_sc = ss_pca.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA.\n",
    "# up to n_components were tested\n",
    "pca = PCA(n_components = 10)\n",
    "\n",
    "# Fit PCA on the training data.\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d5ee",
   "metadata": {},
   "source": [
    "In the below pipeline, PCA will perform unsupervised dimensionality reduction on the training data before upsampling it. This is so that upsampling will only be performed on the reduced dimensional manifold and not on the entire dataset. The models for the prediction will be done last. In the below pipeline, the logistic regression model was utilised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ec555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('logreg', LogisticRegression(max_iter=1_000, solver='saga'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'pca__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, pipe_params, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ff1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py\n",
    "# Plot search for best combination of PCA n_components and logistic regression accuracy\n",
    "# pca.fit(X_train_sc)\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_.cumsum(), '+', linewidth=10)\n",
    "ax0.set_ylabel('Cumulative PCA explained variance ratio')\n",
    "\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components');\n",
    "plt.suptitle('Dimensional reduction based on PCA n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cumulative explained variance (for n_components=9): {pca.explained_variance_ratio_.cumsum()[9]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb630c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67592bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_train, y_train), grid.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea1471",
   "metadata": {},
   "source": [
    "GridSearchCV chose n_components=10, however, there was overfitting at n_components=10. Since PCA utilises n_components, the decision is to use n_components=9, to minimise overfitting, and we would retain approximately 83% of the variability in the data.\n",
    "<br>\n",
    "<br> The curse is after the dataset is PCA-transformed, there is no correlation between any features. And to dig into the feature importance, one will have to convert the PCA-transformed data back to the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b78a82",
   "metadata": {},
   "source": [
    "### Loop models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "models = {'lr': LogisticRegression(max_iter=1000, solver='saga'),\n",
    "        'dt': DecisionTreeClassifier(),\n",
    "        'rf': RandomForestClassifier(),\n",
    "        'et': ExtraTreesClassifier(),\n",
    "        'svc': SVC(probability=True),\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd381257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(model, model_params):\n",
    "    pipe = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('pca', PCA(n_components = 9)),\n",
    "            ('sampling', SMOTE(sampling_strategy = 'minority')),\n",
    "            (model, models[model])\n",
    "            ])\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid=model_params, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # metrics\n",
    "    train_score = grid.score(X_train, y_train)\n",
    "    test_score = grid.score(X_valid, y_valid)\n",
    "    preds = grid.predict(X_valid)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_valid, preds).ravel()\n",
    "\n",
    "#     y_train_pred_prob = pipe.predict_proba(X_train)[:,1]\n",
    "#     y_valid_pred_prob = pipe.predict_proba(X_valid)[:,1]\n",
    "#     train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "#     valid_auc = roc_auc_score(y_valid, y_valid_pred_prob)\n",
    "    \n",
    "    # View confusion matrix\n",
    "    plot_confusion_matrix(grid, X_valid, y_valid, cmap='Blues', values_format='d');\n",
    "    \n",
    "    # Calculate the sensitivity/ recall\n",
    "    sens = tp / (tp + fn)\n",
    "    \n",
    "    # Calculate the specificity\n",
    "    spec = tn / (tn + fp) \n",
    "    \n",
    "    # print results\n",
    "    print(f'Best params: {grid.best_params_}')\n",
    "    print(f'Training score: {round(train_score, 4)}')\n",
    "    print(f'Test score: {round(test_score, 4)}')\n",
    "    print(f'recall: {round(sens, 4)}')\n",
    "    print(f'specificity: {round(spec, 4)}')\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020f3b1",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "grid_params = {\n",
    "    'sampling__k_neighbors': [150, 200, 250],  # recall < 1 above 250\n",
    "    'lr__penalty': ['l1', 'l2',  'elasticnet'],   \n",
    "    'lr__C': [0.1, 1, 10]  \n",
    "} \n",
    "\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [200],  \n",
    "    'lr__penalty': ['l1'],   \n",
    "    'lr__C': [1]  \n",
    "}\n",
    "\n",
    "lr = run_models('lr', best_params)\n",
    "# Best params: {'lr__C': 1, 'lr__penalty': 'l1', 'sampling__k_neighbors': 200, 'sampling__sampling_strategy': 'minority'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefcca1",
   "metadata": {},
   "source": [
    "There was little change in tuning the hyperparameters for the logistic regression. There might be an issue with underfitting, since training score < test score. However, it is uncertain (for now) if one of the ways to combat this is to decrease or increase the sampling strategy, since recall was affected, despite having good training and test scores. 'saga' was selected as the 'solver' since it supports several penalties. 'none' was not provided as an option due to tendency of getting training scores = 1.0. So applying regularisation did help to minimise overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711ecc7",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a76103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "grid_params = {\n",
    "    'sampling__k_neighbors': [150, 200, 250],  \n",
    "    'dt__max_depth': [40, 45, 50],\n",
    "    'dt__min_samples_split': [100, 150, 200],\n",
    "    'dt__min_samples_leaf': [100, 150, 200],\n",
    "} \n",
    "\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [150],  \n",
    "    'dt__max_depth': [50],\n",
    "    'dt__min_samples_split': [150],\n",
    "    'dt__min_samples_leaf': [100],\n",
    "} \n",
    "\n",
    "dt = run_models('dt', best_params)\n",
    "# Best params: {'dt__max_depth': 50, 'dt__min_samples_leaf': 100, 'dt__min_samples_split': 150, 'sampling__k_neighbors': 150, 'sampling__sampling_strategy': 'auto'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f06ef1",
   "metadata": {},
   "source": [
    "I do not understand the popularity of decision trees, due to its tendency to overfit, and efforts spent on tuning the parameters to ensure that training score=1.0 does not occur. The hyperparameters are relatively higher (than what I had previously seen), but this was due to training score = 0.999. 'max_depth' is the maximum depth of the tree, where default=None. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. Since GridSearchCV chooses the estimator based on the highest score, one will have to provide restricted ranges such that training score = 0.999 does not occur. \n",
    "<br> \n",
    "<br> In most parts during hyperparameter selection, there was a balance between providing certain values for GridSearch to tune, such that the higher value of maximum depth does not result in overfitting, and a lower value does not result in underfitting. *(Personal opinion)* higher values should be provided to min_samples_split and min_samples_leaf, however, recall and specificity were penalised/reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c834c2",
   "metadata": {},
   "source": [
    "**Random forest classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "grid_params = {\n",
    "    'sampling__k_neighbors': [150, 200, 250], \n",
    "    'rf__n_estimators': [300, 350, 400],\n",
    "    'rf__max_depth': [10, 15, 20],\n",
    "    'rf__min_samples_split': [100, 150, 200],\n",
    "    'rf__min_samples_leaf': [100, 150, 200],\n",
    "} \n",
    "\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [200], \n",
    "    'rf__n_estimators': [350],\n",
    "    'rf__max_depth': [10],\n",
    "    'rf__min_samples_split': [200],\n",
    "    'rf__min_samples_leaf': [100],\n",
    "} \n",
    "\n",
    "rf = run_models('rf', best_params)\n",
    "# Best params: {'rf__max_depth': 50, 'rf__min_samples_leaf': 100, 'rf__min_samples_split': 200, 'rf__n_estimators': 350, 'sampling__k_neighbors': 200, 'sampling__sampling_strategy': 'auto'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa765833",
   "metadata": {},
   "source": [
    "Ensemble methods are better than simple/one decision tree, due to less probability of overfitting. However, similarly to decision tress (and not totally unexpected), the hyperparameters were relatively higher, but this was due to training score = 0.999. Also (and not totally unexpected), pre-pruning to stop GridSearchCV from growing/cutting down trees was necessary to prevent overfitting. Increasing 'max_depth' would produce better scores; This is not necessary surprising as deeper trees allowed the capture of more information, however, it also increases the likelihood of overfitting. Increasing 'min_samples_leaf' and 'min_samples_split' were essential to ensure that the classifier does not keep splitting the dataset to be more 'pure'. However, there seems to be less issue with recall and specificity in the decision tree and random forests classifiers, compared to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425344d",
   "metadata": {},
   "source": [
    "**Feature importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1612dd",
   "metadata": {},
   "source": [
    "While PCA is not a classifier and one does not necessarily need to PCA-transform data to get good results, it is important to note that PCA **can** be useful when one has a dataset with high dimensionality. Being able to identify the dimensions first before applying a classifier, may help to discriminate between the classes better. Here, the Random Forest classifier is used to determine the gini importance of the n_components=9, and each feature is plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa081be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50796024/feature-variable-importance-after-a-pca-analysis\n",
    "# PCA-transform data back to orignal features\n",
    "num_plot = 9\n",
    "\n",
    "pipe = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('pca', PCA(n_components = num_plot)),\n",
    "            ('sampling', SMOTE(sampling_strategy='minority', k_neighbors=200)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=350, max_depth=50, min_samples_split = 200, min_samples_leaf = 100)),\n",
    "            ])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "model = pipe.steps[1][1]\n",
    "n_pcs = model.components_.shape[0]\n",
    "\n",
    "# get index for more important feature from each n_component\n",
    "feature_names = X_train.columns\n",
    "most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n",
    "most_important_names = [feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# combine for plotting later\n",
    "zipped_feats = zip(most_important_names, pipe.steps[3][1].feature_importances_)\n",
    "zipped_feats = sorted(zipped_feats, key=lambda x: x[1], reverse=True)\n",
    "features, importances = zip(*zipped_feats)\n",
    "\n",
    "# n_components = 9\n",
    "top_features = features[:num_plot]\n",
    "top_importances = importances[:num_plot]\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.barh(range(len(top_importances)), top_importances, align='center')\n",
    "plt.yticks(range(len(top_importances)), top_features)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance attribute of each prinicipal component\n",
    "pipe.steps[1][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc129f",
   "metadata": {},
   "source": [
    "The above plots show the nine highest performing predictors. The explained variance ratio provides the information (variance) that is captured by each principal component. So, 'year' explains approximately 28% of the variance in the dataset. 'Sunrise' explains approximately 17% of the variance in the dataset. As demonstrated earlier, n_components=9, could explain approximate 80% of the variability in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9bd995",
   "metadata": {},
   "source": [
    "**year** was a surprise feature. And on hindsight, should have been dropped, if not because we were relying on PCA to transform our entire dataset. How does the year explain 28% of the variability in the dataset? It could be that learning from the past, will bring about wisdom and understanding in the future. During EDA, it was shown that years 2007 and 2013 are years with the highest presence wnv detected. In 2007, a large amount of rain and hot weather produced favourable conditions for eggs ([source](https://www.cmaj.ca/content/177/12/1489.1)). In 2012/2013, was the deadliest year for the wnv in the US due to higher than normal temperatures, and sustained interactions between mosquito and birds (which are reservoir hosts for wnv) ([source](https://www.nbcnews.com/healthmain/2012-was-deadliest-year-west-nile-us-cdc-says-1c9904312))\n",
    "<br>\n",
    "<br> **Sunrise** was not a surprise since mosquitoes are most active around the time of sunrise ([source](https://portal.ct.gov/Mosquito/Press-Room/2020-Press-Releases/DPH-Announces-Three-New-Cases-of-West-Nile-Virus-Infection-in-Fairfield-County.)). \n",
    "<br>\n",
    "<br> **lag_2_preciptotal** and **preciptotal** were not surprising features either, since increased precipitation have a lagged effect on wnvpresent ([source](https://pubmed.ncbi.nlm.nih.gov/30145430/)). However, we were expecting more lagged features from temperatures as well.\n",
    "<br>\n",
    "<br> **wetbulb** was also discussed in EDA. Since wetbulb takes into account both precipitation and temperature, it has demonstrated to have a higher predictive power since it is a feature engineered through combining temperature and precipitation ([source](https://www.theweatherprediction.com/habyhints/259/)).\n",
    "<br>\n",
    "<br> **resultdir** is the resultant wind direction, measured to whole degrees, based on a 360 degree compass (with 0 or 360 degrees from the North). **avgspeed** is the average wind speed. While one would expect higher temperatures and rainfall to dominant the feature importances, it seems *where* and *how fast* the wind is blowing could also be important features. This could be related to FG (fog column)([source](https://www.sciencedaily.com/releases/2012/11/121119104522.htm)), since the spread of wnv requires the mosquitoes to be well conditioned or aided (by wind) for flight.\n",
    "<br>\n",
    "<br> We have discussed the top 2 **species_nr** culex pipiens, culex restuans that are responsible for the spread of the wnv in our EDA.\n",
    "<br>\n",
    "<br> **longitude** is related to location and possibly where the traps were being placed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539f7d9",
   "metadata": {},
   "source": [
    "**Extra trees classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f48f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra trees classifier\n",
    "grid_params = {\n",
    "    'sampling__k_neighbors': [15, 20, 25],\n",
    "    'et__n_estimators': [30, 35],\n",
    "    'et__max_depth': [4, 5, 6],\n",
    "    'et__min_samples_split': [10, 15],\n",
    "    'et__min_samples_leaf': [10, 15],\n",
    "} \n",
    "\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [20],\n",
    "    'et__n_estimators': [30],  \n",
    "    'et__max_depth': [4],  \n",
    "    'et__min_samples_split': [15],\n",
    "    'et__min_samples_leaf': [10],\n",
    "}\n",
    "\n",
    "et = run_models('et', best_params)\n",
    "# Best params: {'et__max_depth': 450, 'et__min_samples_leaf': 100, 'et__min_samples_split': 150, 'et__n_estimators': 300, 'sampling__k_neighbors': 200, 'sampling__sampling_strategy': 'minority'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd14440",
   "metadata": {},
   "source": [
    "Extra trees implements a meta estimator that fits a number of randomized decision trees on various sub-samples of the dataset. It should improve the accuracy while control for over-fitting. So far, this has been true; The number of trees are fewer, and do not need to be deep (max_depth) to produce good scores. Increasing the n_estimators also increased the accuracy, but up to a certain point (in this case, 30 trees). min_samples_split and min_samples_leaf do not have to be large to avoid overfitting. Feature importance was not performed because n_components=9 was used for all the models using PCA-transformed data, and they are the same 9 features. *(Preliminary analysis found that sunrise and year would fight to be top dog in the different models, but that was just a small difference)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9548a11",
   "metadata": {},
   "source": [
    "**Support Vector Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfe9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classification\n",
    "grid_params = {\n",
    "    'sampling__k_neighbors': [250, 300],     \n",
    "    'svc__kernel': ['linear', 'sigmoid'],   \n",
    "    'svc__gamma' : ['scale', 'auto'], \n",
    "    'svc__C': [1 ,5 ,10] \n",
    "} \n",
    "\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [250],     \n",
    "    'svc__kernel': ['linear'],    \n",
    "    'svc__gamma' : ['scale'],\n",
    "    'svc__C': [1] \n",
    "} \n",
    "\n",
    "svc = run_models('svc',  best_params)\n",
    "# Best params: {'sampling__k_neighbors': 250, 'svc__C': 1, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e3f7b",
   "metadata": {},
   "source": [
    "In the above, hyperparameters tuned were the kernel, gamma and regularization parameter. 'rbf' and 'poly' were not provided as options, as there were instances where training score=0.99 (dependent on other parameters). Choosing an optimal gamma was important too, since the parameter controls overfitting; The higher the gamma, the higher the hyperplane to match the training data. A regularization parameter C represents the tradeoff between a smoother hyperplane and misclassifications. In summary, SVM is known to work well with non-linear data, is simple to implement, and provided high accuracy. However, it requires a lot of training time compared to other algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acf287",
   "metadata": {},
   "source": [
    "### Plot ROC-AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76159b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_auc(models):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    axes = {}\n",
    "    for i, m in enumerate(models.keys()):\n",
    "        axes[f'ax{i}'] = plot_roc_curve(m, X_valid, y_valid, ax=ax, name=models[m])\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], color='k', linestyle='--')\n",
    "    plt.title('ROC-AUC curve')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_built = {\n",
    "    lr : 'LogisticRegression', \n",
    "    dt : 'DecisionTreeClassifier',\n",
    "    rf : 'RandomForestClassifier',\n",
    "    et : 'ExtraTreesClassifier',\n",
    "    svc : 'SVC',\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_auc(models_built)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c076c",
   "metadata": {},
   "source": [
    "From the above ROC-AUC curve, the Random Forest classifier has the highest AUC score, ie., it is better at predicting classes or distinguishing between the classes. So the Random Forest model also presents the best performing trade-off between the true positives and true negatives. In addition, the model has a recall rate at 0.83, and specificity at 0.94. Therefore, using PCA-transformed data, **the Random Forest model was selected as the chosen model.**\n",
    "<br>\n",
    "<br> At FPR=1, where sensitivity is the highest, all classifiers seemed to performed well till FPR=0.8. Below this point, the decision tree (dt) classifier starts to exhibit lower sensitivity, with the lowest sensitivity reported among all the models. Given the tendency for dt to overfit, the parameters were tuned such that a training score = 0.999 did not occur, however, sensitivity was also penalised. \n",
    "<br> \n",
    "<br> At FPR=0, where specificity is the highest, the sudden decrease in logistic regression showed that the model, together with SVC have the lowest specificity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1d102",
   "metadata": {},
   "source": [
    "## Kaggle predictions using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997be836",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "X_test = test.drop(columns=['id','date'])\n",
    "\n",
    "# check that shape is same to train dataset\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=['date','wnvpresent','nummosquitos'])\n",
    "y = train[['wnvpresent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, model_params):\n",
    "    pipe = Pipeline([\n",
    "            ('scale', StandardScaler()),\n",
    "            ('pca', PCA(n_components = 9)),\n",
    "            ('sampling', SMOTE()),\n",
    "            (model, models[model])\n",
    "            ])\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid=model_params, scoring='roc_auc', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    train_score = grid.score(X, y)\n",
    "#     preds = grid.predict(X_test)\n",
    "    pred_prob = grid.predict_proba(X_test)\n",
    "    \n",
    "    print(f'Training score: {round(train_score, 4)}')\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da38f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "best_params = {\n",
    "    'sampling__k_neighbors': [200], \n",
    "    'rf__n_estimators': [350],\n",
    "    'rf__max_depth': [10],\n",
    "    'rf__min_samples_split': [200],\n",
    "    'rf__min_samples_leaf': [100],\n",
    "} \n",
    "\n",
    "y_pred = run_test('rf', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y\n",
    "kaggle_y = test[['id']]\n",
    "kaggle_y['wnvpresent'] = pd.DataFrame(y_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb6cb7",
   "metadata": {},
   "source": [
    "**Save predicted results as csv file for kaggle submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_y.to_csv('../data/kaggle_y.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7741b",
   "metadata": {},
   "source": [
    "### <font color = red> Predicted locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Still needs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('../data/gs_ss_smote_logit_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_merge = pd.merge(test, predictions, left_on='id', right_on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc443f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_merge['WnvPresent'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f167ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_merge['longitude'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31358d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/predict-west-nile-virus/\n",
    "# This shows how to read the text representing a map of Chicago in numpy, and put it on a plot in matplotlib.\n",
    "# This example also rescales the image data to the GPS co-ordinates of the bounding box and overlays some random points.\n",
    "\n",
    "origin = [41.6, -88.0]              # lat/long of origin (lower left corner)\n",
    "upperRight = [42.1, -87.5]          # lat/long of upper right corner\n",
    "\n",
    "mapdata = np.loadtxt('../data/mapdata_copyright_openstreetmap_contributors.txt')\n",
    "\n",
    "# generate some data to overlay\n",
    "numPoints = 300\n",
    "lats = predict_merge['latitude']\n",
    "longs = predict_merge['longitude']\n",
    "wnv = predict_merge['WnvPresent']\n",
    "lats_traps = train['latitude']\n",
    "longs_traps = train['longitude']\n",
    "\n",
    "# #normalise colormap\n",
    "# colormap = plt.cm.blues #or any other colormap\n",
    "# normalize = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "\n",
    "# generate plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mapdata, cmap=plt.get_cmap('gray'), extent=[origin[1], upperRight[1], origin[0], upperRight[0]])\n",
    "# sns.scatterplot(x=longs, y=lats, palette='husl', \n",
    "#                 c='k', \n",
    "#                 hue=sns.wnv, \n",
    "#                 s=40)\n",
    "\n",
    "# plot colour based on probability values\n",
    "# sns.scatterplot(x=longs, y=lats, \n",
    "# #                 c='r',\n",
    "#                 hue=wnv, size=wnv,\n",
    "#                 cmap=plt.cm.Blues,\n",
    "#                 hue_norm=(0,255),\n",
    "#                 s=100)\n",
    "plt.scatter(longs,lats, c=wnv, cmap='gray', norm=matplotlib.colors.Normalize(vmin=0, vmax=1))\n",
    "plt.colorbar()\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('map_predict.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d700a29",
   "metadata": {},
   "source": [
    "### <font color=red> Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533cb4e",
   "metadata": {},
   "source": [
    "(Model) was selected due to several reasons:\n",
    "*\n",
    "*\n",
    "* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60152265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5c727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "243fffab",
   "metadata": {},
   "source": [
    "### <font color=red>Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4cc7c",
   "metadata": {},
   "source": [
    "* New features such as humidity or sunlight duration should be added into the dataset.\n",
    "* Traps will not be ranked, but dummified, or ranked according to number of mosquitoes caught. Ranking of the traps by groups resulted in overfitting in our models.\n",
    "* Year could be dropped. Despite year being one of the feature importance in our model (after PCA-transform), the focus should be on weather conditions which will impede/encourage mosquitoes breeding and proliferation, rather than seasonal changes, even if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ed9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
